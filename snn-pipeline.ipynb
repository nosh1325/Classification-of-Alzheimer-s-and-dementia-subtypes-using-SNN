{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12057859,"sourceType":"datasetVersion","datasetId":7589076},{"sourceId":13260399,"sourceType":"datasetVersion","datasetId":8402875}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install snntorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T13:13:24.728717Z","iopub.execute_input":"2025-10-11T13:13:24.729380Z","iopub.status.idle":"2025-10-11T13:13:30.312778Z","shell.execute_reply.started":"2025-10-11T13:13:24.729352Z","shell.execute_reply":"2025-10-11T13:13:30.312031Z"}},"outputs":[{"name":"stdout","text":"Collecting snntorch\n  Downloading snntorch-0.9.4-py2.py3-none-any.whl.metadata (15 kB)\nDownloading snntorch-0.9.4-py2.py3-none-any.whl (125 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: snntorch\nSuccessfully installed snntorch-0.9.4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, random_split\nfrom sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\nimport snntorch as snn\nfrom snntorch import spikegen\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install snntorch torchvision matplotlib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T13:13:41.958980Z","iopub.execute_input":"2025-10-11T13:13:41.959408Z","iopub.status.idle":"2025-10-11T13:14:58.597173Z","shell.execute_reply.started":"2025-10-11T13:13:41.959385Z","shell.execute_reply":"2025-10-11T13:14:58.596229Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: snntorch in /usr/local/lib/python3.11/dist-packages (0.9.4)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (confusion_matrix, classification_report, f1_score, \n                           precision_score, recall_score, accuracy_score)\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.feature_selection import SelectKBest, f_classif, RFE, VarianceThreshold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport cv2\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nTIME_STEPS = 20\nBATCH_SIZE = 64\nLEARNING_RATE = 0.008\nEPOCHS = 10  \nIMG_SIZE = 32\nINPUT_SIZE = IMG_SIZE * IMG_SIZE\nFAST_TRAINING_BATCHES = 20\nFAST_TRAIN_FEATURES = 50\n\nprint(f\"Training batches: {FAST_TRAINING_BATCHES}, Feature batches: {FAST_TRAIN_FEATURES}, Epochs: {EPOCHS}\")\n\nimages = []\nlabels = []\n\ndef load_datasets(original_path, additional_path=None):\n    images = []\n    labels = []\n    \n    for subfolder in os.listdir(original_path):\n        subfolder_path = os.path.join(original_path, subfolder)\n        for folder in os.listdir(subfolder_path):\n            subfolder_path2 = os.path.join(subfolder_path, folder)\n            for image_filename in os.listdir(subfolder_path2):\n                if image_filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n                    image_path = os.path.join(subfolder_path2, image_filename)\n                    images.append(image_path)\n                    labels.append(folder)\n    \n    if additional_path and os.path.exists(additional_path):\n        for class_folder in ['Demented', 'NonDemented']:\n            class_path = os.path.join(additional_path, class_folder)\n            if os.path.exists(class_path):\n                items = os.listdir(class_path)\n                for item in items:\n                    item_path = os.path.join(class_path, item)\n                    if os.path.isdir(item_path):\n                        for image_file in os.listdir(item_path):\n                            if image_file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n                                image_path = os.path.join(item_path, image_file)\n                                images.append(image_path)\n                                labels.append(class_folder)\n                    elif item.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n                        images.append(item_path)\n                        labels.append(class_folder)\n    \n    return images, labels\n\nBASE_PATH = '/kaggle/input/alzheimerdataset'\nADDITIONAL_PATH = '/kaggle/input/nins-dementia-v2'\n\nimages, labels = load_datasets(BASE_PATH, ADDITIONAL_PATH)\ndf = pd.DataFrame({'image': images, 'label': labels})\n\ndef create_binary_labels(label):\n    return 0 if label == \"NonDemented\" else 1\n\ndf['label_idx'] = df['label'].apply(create_binary_labels)\nlabel_to_idx = {'NonDemented': 0, 'Demented': 1}\n\nprint(f\"Total samples: {len(df)}\")\n\nclass AlzheimerDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.df = dataframe\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image_path = self.df.iloc[idx]['image']\n        label = self.df.iloc[idx]['label_idx']\n        image = Image.open(image_path).convert(\"L\")\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\nclass ImagePreprocessor:\n    def __init__(self, time_steps=20, img_size=32):\n        self.time_steps = time_steps\n        self.img_size = img_size\n        \n    def preprocess_scan(self, image_tensor):\n        device = image_tensor.device\n        batch_size = image_tensor.shape[0]\n        processed_images = []\n        \n        for i in range(batch_size):\n            img = image_tensor[i].cpu().numpy().reshape(self.img_size, self.img_size)\n            img_8bit = (img * 255).astype(np.uint8)\n            \n            blurred = cv2.GaussianBlur(img_8bit, (5, 5), 0)\n            _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n            \n            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n            cleaned = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n            cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_CLOSE, kernel)\n            \n            contours, _ = cv2.findContours(cleaned, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            if contours:\n                largest_contour = max(contours, key=cv2.contourArea)\n                mask = np.zeros_like(img_8bit)\n                cv2.drawContours(mask, [largest_contour], -1, 255, -1)\n                skull_stripped = cv2.bitwise_and(img_8bit, mask)\n            else:\n                skull_stripped = img_8bit\n            \n            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n            enhanced = clahe.apply(skull_stripped)\n            \n            processed_img = torch.from_numpy(enhanced / 255.0).float().to(device)\n            processed_images.append(processed_img.flatten())\n        \n        return torch.stack(processed_images, dim=0)\n    \n    def encode_temporal(self, data):\n        device = data.device\n        batch_size, features = data.shape\n        \n        data_norm = torch.clamp(data, 0, 1)\n        spikes = torch.zeros(self.time_steps, batch_size, features, device=device)\n        \n        for b in range(batch_size):\n            for f in range(features):\n                intensity = data_norm[b, f].item()\n                if intensity > 0.1:\n                    spike_time = int((1 - intensity) * (self.time_steps - 1))\n                    spike_time = max(0, min(spike_time, self.time_steps - 1))\n                    spikes[spike_time, b, f] = 1.0\n                    \n                    if intensity > 0.8 and spike_time + 1 < self.time_steps:\n                        spikes[spike_time + 1, b, f] = 0.5\n        \n        return spikes\n    \n    def extract_spike_features(self, spikes):\n        features = []\n        \n        features.append(spikes.mean(dim=0))\n        features.append(spikes.sum(dim=0))\n        features.append(spikes.std(dim=0))\n        features.append(spikes.max(dim=0)[0])\n        \n        batch_size, pixel_features = spikes.shape[1], spikes.shape[2]\n        \n        first_spike = torch.zeros(batch_size, pixel_features, device=spikes.device)\n        for b in range(batch_size):\n            for f in range(pixel_features):\n                spike_times = torch.nonzero(spikes[:, b, f])\n                if len(spike_times) > 0:\n                    first_spike[b, f] = spike_times[0].float() / self.time_steps\n                else:\n                    first_spike[b, f] = 1.0\n        features.append(first_spike)\n        \n        burst_features = torch.zeros_like(spikes[0])\n        for t in range(1, self.time_steps):\n            consecutive_spikes = spikes[t-1] * spikes[t]\n            burst_features += consecutive_spikes\n        features.append(burst_features)\n        \n        phase1 = spikes[:self.time_steps//2].mean(dim=0)\n        phase2 = spikes[self.time_steps//2:].mean(dim=0)\n        features.extend([phase1, phase2])\n        \n        return torch.cat(features, dim=1)\n    \n    def __call__(self, batch_data, add_noise=False):\n        device = batch_data.device\n        \n        skull_stripped = self.preprocess_scan(batch_data)\n        temporal_spikes = self.encode_temporal(skull_stripped)\n        temporal_features = self.extract_spike_features(temporal_spikes)\n        \n        combined_features = temporal_features\n        combined_features = torch.nan_to_num(combined_features, 0.0)\n        combined_features = torch.clamp(combined_features, -10, 10)\n        \n        if add_noise:\n            noise = torch.randn_like(combined_features) * 0.02\n            noisy_features = combined_features + noise\n            return noisy_features, combined_features\n        \n        return combined_features, combined_features\n\nclass FeatureAutoencoder(nn.Module):\n    def __init__(self, input_size, time_steps, latent_dim=256):\n        super(FeatureAutoencoder, self).__init__()\n        self.input_size = input_size\n        self.time_steps = time_steps\n        self.latent_dim = latent_dim\n        \n        self.encoder1 = nn.Linear(input_size, 512, bias=True)\n        self.encoder2 = nn.Linear(512, 256, bias=True)\n        self.bottleneck = nn.Linear(256, self.latent_dim, bias=True)\n        self.decoder1 = nn.Linear(self.latent_dim, 256, bias=True)\n        self.decoder2 = nn.Linear(256, 512, bias=True)\n        self.decoder3 = nn.Linear(512, input_size, bias=True)\n        \n        self.threshold = 1.5\n        self.ff_learning_rate = 0.01\n        \n        self.tau_pre = 20.0\n        self.tau_post = 20.0\n        self.A_plus = 0.02\n        self.A_minus = 0.01\n        \n        self.reward_baseline = 0.0\n        self.reward_lr = 0.2\n        self.layer_norms = [0.3, 0.25, 0.2]\n        \n        self.init_weights()\n    \n    def init_weights(self):\n        for i, layer in enumerate([self.encoder1, self.encoder2, self.bottleneck]):\n            nn.init.normal_(layer.weight, mean=0.0, std=self.layer_norms[i])\n            if layer.bias is not None:\n                nn.init.normal_(layer.bias, mean=0.0, std=0.1)\n        for layer in [self.decoder1, self.decoder2, self.decoder3]:\n            nn.init.normal_(layer.weight, mean=0.0, std=0.2)\n            if layer.bias is not None:\n                nn.init.normal_(layer.bias, mean=0.0, std=0.1)\n    \n    def encode(self, input_features):\n        with torch.no_grad():\n            x1 = torch.relu(self.encoder1(input_features))\n            x2 = torch.relu(self.encoder2(x1))\n            z_m_raw = self.bottleneck(x2)\n            z_m = torch.sigmoid(z_m_raw)\n            return z_m.detach()\n    \n    def decode(self, z_m):\n        with torch.no_grad():\n            x = torch.relu(self.decoder1(z_m))\n            x = torch.relu(self.decoder2(x))\n            reconstruction = self.decoder3(x)\n            return reconstruction\n    \n    def get_embeddings(self, dataloader, preprocessor, max_batches=None):\n        embeddings = []\n        labels_list = []\n        \n        self.eval()\n        with torch.no_grad():\n            for batch_idx, (data, labels) in enumerate(tqdm(dataloader, desc=\"Extracting embeddings\")):\n                if max_batches is not None and batch_idx >= max_batches:\n                    break\n                    \n                data = data.to(device)\n                combined_features, _ = preprocessor(data)\n                combined_features = combined_features.to(device)\n                \n                z_m = self.encode(combined_features)\n                embeddings.append(z_m.cpu())\n                labels_list.append(labels)\n        \n        return torch.cat(embeddings, dim=0), torch.cat(labels_list, dim=0)\n    \n    def forward(self, input_features):\n        x1 = torch.relu(self.encoder1(input_features))\n        x2 = torch.relu(self.encoder2(x1))\n        z_m_raw = self.bottleneck(x2)\n        z_m = torch.sigmoid(z_m_raw)\n        \n        d1 = torch.relu(self.decoder1(z_m))\n        d2 = torch.relu(self.decoder2(d1))\n        reconstruction = self.decoder3(d2)\n        \n        return reconstruction, z_m\n    \n    def create_samples(self, x, noise_level=0.2):\n        batch_size = x.shape[0]\n        positive_samples = x.clone()\n        negative_samples = x.clone()\n        \n        noise = torch.randn_like(x) * noise_level * 2.0\n        negative_samples += noise\n        \n        for i in range(batch_size):\n            n_shuffle = int(0.4 * x.shape[1])\n            shuffle_idx = torch.randperm(x.shape[1])[:n_shuffle]\n            random_sample_idx = torch.randint(0, batch_size, (1,)).item()\n            negative_samples[i, shuffle_idx] = x[random_sample_idx, shuffle_idx]\n        \n        invert_mask = torch.rand_like(x) < 0.2\n        negative_samples[invert_mask] = 1.0 - negative_samples[invert_mask]\n        \n        random_mask = torch.rand_like(x) < 0.1\n        negative_samples[random_mask] = torch.rand_like(negative_samples[random_mask])\n        \n        combined_samples = torch.cat([positive_samples, negative_samples], dim=0)\n        labels = torch.cat([torch.ones(batch_size), torch.zeros(batch_size)], dim=0)\n        \n        return combined_samples, labels\n    \n    def update_weights_direct(self, features):\n        with torch.no_grad():\n            x1 = torch.relu(self.encoder1(features))\n            if x1.sum() < 1e-6:\n                self.encoder1.weight.data += torch.randn_like(self.encoder1.weight) * 0.1\n                self.encoder1.bias.data += torch.randn_like(self.encoder1.bias) * 0.1\n            \n            x2 = torch.relu(self.encoder2(x1))\n            if x2.sum() < 1e-6:\n                self.encoder2.weight.data += torch.randn_like(self.encoder2.weight) * 0.1\n                self.encoder2.bias.data += torch.randn_like(self.encoder2.bias) * 0.1\n\n            z_raw = self.bottleneck(x2)\n            if z_raw.std() < 1e-6:\n                self.bottleneck.weight.data += torch.randn_like(self.bottleneck.weight) * 0.15\n                self.bottleneck.bias.data += torch.randn_like(self.bottleneck.bias) * 0.15\n    \n    def update_layer(self, layer, x_pos, x_neg, layer_idx):\n        with torch.no_grad():\n            h_pos = layer(x_pos)\n            h_neg = layer(x_neg)\n            \n            if layer_idx < 2:\n                h_pos_act = torch.relu(h_pos)\n                h_neg_act = torch.relu(h_neg)\n            else:\n                h_pos_act = torch.sigmoid(h_pos)\n                h_neg_act = torch.sigmoid(h_neg)\n            \n            goodness_pos = torch.sum(h_pos_act ** 2, dim=1)\n            goodness_neg = torch.sum(h_neg_act ** 2, dim=1)\n            \n            pos_loss = torch.relu(self.threshold - goodness_pos) * 2.0\n            neg_loss = torch.relu(goodness_neg - self.threshold) * 2.0\n\n            total_grad = torch.zeros_like(layer.weight)\n            bias_grad = torch.zeros_like(layer.bias)\n            \n            for i in range(len(x_pos)):\n                if pos_loss[i] > 0:\n                    grad = torch.outer(h_pos_act[i], x_pos[i]) * pos_loss[i].item()\n                    total_grad += grad * 3.0\n                    bias_grad += h_pos_act[i] * pos_loss[i].item() * 3.0\n            \n            for i in range(len(x_neg)):\n                if neg_loss[i] > 0:\n                    grad = torch.outer(h_neg_act[i], x_neg[i]) * neg_loss[i].item()\n                    total_grad -= grad * 3.0\n                    bias_grad -= h_neg_act[i] * neg_loss[i].item() * 3.0\n\n            layer.weight.data += self.ff_learning_rate * (total_grad / (len(x_pos) + len(x_neg)))\n            layer.bias.data += self.ff_learning_rate * (bias_grad / (len(x_pos) + len(x_neg)))\n            \n            layer.weight.data = torch.clamp(layer.weight.data, -0.5, 0.5)\n            layer.bias.data = torch.clamp(layer.bias.data, -0.3, 0.3)\n            \n            return goodness_pos.mean(), goodness_neg.mean()\n    \n    def boost_weights(self):\n        with torch.no_grad():\n            self.encoder1.weight.data *= 1.5\n            self.encoder1.bias.data += torch.randn_like(self.encoder1.bias) * 0.2\n            \n            self.encoder2.weight.data *= 1.5\n            self.encoder2.bias.data += torch.randn_like(self.encoder2.bias) * 0.2\n            \n            self.bottleneck.weight.data *= 2.0\n            self.bottleneck.bias.data += torch.randn_like(self.bottleneck.bias) * 0.3\n    \n    def train_model(self, data_loader, preprocessor, epochs=12):\n        for epoch in range(epochs):\n            print(f\"Epoch {epoch+1}/{epochs}\")\n            \n            batch_count = 0\n            for batch_idx, (data, labels) in enumerate(data_loader):\n                if batch_idx >= FAST_TRAINING_BATCHES:\n                    break\n                \n                data = data.to(device)\n                features, _ = preprocessor(data)\n                features = features.to(device)\n                \n                if batch_idx < 5:\n                    self.update_weights_direct(features)\n                \n                combined_samples, sample_labels = self.create_samples(features)\n                pos_samples = combined_samples[sample_labels == 1]\n                neg_samples = combined_samples[sample_labels == 0]\n                \n                current_pos = pos_samples\n                current_neg = neg_samples\n                \n                layers = [self.encoder1, self.encoder2, self.bottleneck]\n                layer_goodness_pos = []\n                layer_goodness_neg = []\n                \n                for layer_idx, layer in enumerate(layers):\n                    goodness_pos, goodness_neg = self.update_layer(\n                        layer, current_pos, current_neg, layer_idx\n                    )\n                    \n                    layer_goodness_pos.append(goodness_pos)\n                    layer_goodness_neg.append(goodness_neg)\n                    \n                    with torch.no_grad():\n                        if layer_idx < len(layers) - 1:\n                            current_pos = torch.relu(layer(current_pos))\n                            current_neg = torch.relu(layer(current_neg))\n                        else:\n                            current_pos = torch.sigmoid(layer(current_pos))\n                            current_neg = torch.sigmoid(layer(current_neg))\n                \n                with torch.no_grad():\n                    test_z_m = self.encode(pos_samples[:4])\n                    z_m_std = test_z_m.std().item()\n                    \n                    if z_m_std < 1e-6:\n                        self.boost_weights()\n                \n                batch_count += 1\n            \n            if epoch % 3 == 0:\n                print(f\"  Processed {batch_count} batches\")\n\ndef extract_features(autoencoder, dataloader, preprocessor, max_batches=None, dataset_type=\"unknown\"):\n    all_features = []\n    all_labels = []\n    \n    desc = f\"Extracting {dataset_type} features\"\n    if max_batches:\n        desc += f\" ({max_batches} batches)\"\n    else:\n        desc += \" (full dataset)\"\n    \n    autoencoder.eval()\n    with torch.no_grad():\n        for batch_idx, (data, labels) in enumerate(tqdm(dataloader, desc=desc)):\n            if max_batches is not None and batch_idx >= max_batches:\n                break\n            \n            data = data.to(device)\n            combined_features, _ = preprocessor(data)\n            combined_features = combined_features.to(device)\n            \n            reconstruction, bottleneck_features = autoencoder(combined_features)\n            \n            final_features = torch.cat([combined_features, bottleneck_features], dim=1)\n            \n            all_features.append(final_features.cpu())\n            all_labels.append(labels)\n    \n    features_tensor = torch.cat(all_features, dim=0)\n    labels_tensor = torch.cat(all_labels, dim=0)\n    \n    return features_tensor, labels_tensor\n\ndef select_features(train_features, train_labels, test_features, target_features=3000):\n    print(f\"Original features: {train_features.shape[1]}\")\n    X_train = train_features.numpy()\n    X_test = test_features.numpy()\n    y_train = train_labels.numpy()\n    \n    X_train = np.nan_to_num(X_train, 0.0)\n    X_test = np.nan_to_num(X_test, 0.0)\n    \n    variance_selector = VarianceThreshold(threshold=0.005)\n    X_train_var = variance_selector.fit_transform(X_train)\n    X_test_var = variance_selector.transform(X_test)\n    \n    k_best = min(target_features * 3, X_train_var.shape[1])\n    stat_selector = SelectKBest(score_func=f_classif, k=k_best)\n    X_train_stat = stat_selector.fit_transform(X_train_var, y_train)\n    X_test_stat = stat_selector.transform(X_test_var)\n    \n    rf_selector = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n    rfe_selector = RFE(estimator=rf_selector, n_features_to_select=target_features, step=300)\n    X_train_final = rfe_selector.fit_transform(X_train_stat, y_train)\n    X_test_final = rfe_selector.transform(X_test_stat)\n    print(f\"After RFE: {X_train_final.shape[1]}\")\n    \n    return (X_train_final, X_test_final, \n            {'variance': variance_selector, 'statistical': stat_selector, 'rfe': rfe_selector})\n\ndef train_classifier(X_train, y_train, X_test, y_test):\n    X_train = np.nan_to_num(X_train, 0.0)\n    X_test = np.nan_to_num(X_test, 0.0)\n    \n    scaler = RobustScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    rf = RandomForestClassifier(\n        n_estimators=600,\n        max_depth=25,\n        min_samples_split=2,\n        min_samples_leaf=1,\n        max_features='sqrt',\n        class_weight='balanced',\n        random_state=42,\n        oob_score=True,\n        n_jobs=-1\n    )\n    rf.fit(X_train_scaled, y_train)\n    rf_oob_score = rf.oob_score_\n    \n    gb = GradientBoostingClassifier(\n        n_estimators=150,\n        max_depth=6,\n        learning_rate=0.01,\n        subsample=0.85,\n        random_state=42,\n        validation_fraction=0.15,\n        n_iter_no_change=12\n    )\n    gb.fit(X_train_scaled, y_train)\n    \n    ensemble = VotingClassifier([\n        ('rf', rf),\n        ('gb', gb)\n    ], voting='soft')\n    ensemble.fit(X_train_scaled, y_train)\n    \n    y_pred = ensemble.predict(X_test_scaled)\n    y_pred_proba = ensemble.predict_proba(X_test_scaled)\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    precision_weighted = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n    precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)\n    recall_weighted = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n    f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n    \n    rf_pred = rf.predict(X_test_scaled)\n    gb_pred = gb.predict(X_test_scaled)\n    rf_accuracy = accuracy_score(y_test, rf_pred)\n    gb_accuracy = accuracy_score(y_test, gb_pred)\n    \n    return ensemble, scaler, {\n        'accuracy': accuracy,\n        'precision_weighted': precision_weighted,\n        'precision_macro': precision_macro,\n        'recall_weighted': recall_weighted,\n        'f1_weighted': f1_weighted,\n        'predictions': y_pred,\n        'probabilities': y_pred_proba,\n        'oob_score': rf_oob_score,\n        'rf_accuracy': rf_accuracy,\n        'gb_accuracy': gb_accuracy\n    }\n\ndef plot_results(y_true, y_pred, class_names, results, save_path=None):\n    cm = confusion_matrix(y_true, y_pred)\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n    \n    precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)\n    recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)\n    f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)\n    accuracy_per_class = []\n    \n    for i in range(len(class_names)):\n        true_pos = cm[i, i]\n        total_actual = cm[i, :].sum()\n        accuracy_per_class.append(true_pos / total_actual if total_actual > 0 else 0)\n    \n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n    \n    total_samples = cm.sum()\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names, ax=ax1)\n    ax1.set_title(f'Confusion Matrix ({total_samples:,} samples)\\nAccuracy: {results[\"accuracy\"]:.3f}')\n    ax1.set_xlabel('Predicted')\n    ax1.set_ylabel('True')\n    \n    sns.heatmap(cm_normalized, annot=True, fmt='.1f', cmap='Reds',\n                xticklabels=class_names, yticklabels=class_names, ax=ax2)\n    ax2.set_title('Confusion Matrix (Percentages)')\n    ax2.set_xlabel('Predicted')\n    ax2.set_ylabel('True')\n    \n    x = np.arange(len(class_names))\n    width = 0.2\n    \n    ax3.bar(x - width*1.5, precision_per_class, width, label='Precision', alpha=0.8, color='blue')\n    ax3.bar(x - width*0.5, recall_per_class, width, label='Recall', alpha=0.8, color='orange')\n    ax3.bar(x + width*0.5, f1_per_class, width, label='F1-Score', alpha=0.8, color='green')\n    ax3.bar(x + width*1.5, accuracy_per_class, width, label='Class Accuracy', alpha=0.8, color='red')\n    \n    ax3.set_xlabel('Classes')\n    ax3.set_ylabel('Score')\n    ax3.set_title('Per-Class Performance Metrics')\n    ax3.set_xticks(x)\n    ax3.set_xticklabels(class_names, rotation=45)\n    ax3.legend()\n    ax3.set_ylim(0, 1)\n    \n    for i, (p, r, f, a) in enumerate(zip(precision_per_class, recall_per_class, f1_per_class, accuracy_per_class)):\n        ax3.text(i-width*1.5, p+0.01, f'{p:.3f}', ha='center', va='bottom', fontsize=8)\n        ax3.text(i-width*0.5, r+0.01, f'{r:.3f}', ha='center', va='bottom', fontsize=8)\n        ax3","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}